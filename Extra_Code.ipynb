{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9jgZfe8mAFI"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/drive/My Drive/Colab Notebooks/corpus.pkl','rb') as f:\n",
    "    corpus=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tSUYTNUMmXll",
    "outputId": "7e041a82-4114-48e5-83f4-8d090051120a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>735454</td>\n",
       "      <td>Ate lunch here today for the first time. Food ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>735423</td>\n",
       "      <td>This place was kinda hard to find. But after w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>734716</td>\n",
       "      <td>For the summer, my brother is lucky enough to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>733451</td>\n",
       "      <td>Randomly stopped in here last night (Thursday)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>733696</td>\n",
       "      <td>YUM this place is so goooood. I only recently ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  ordinal                                         reviewText  label\n",
       "0      3.0   735454  Ate lunch here today for the first time. Food ...      0\n",
       "1      4.0   735423  This place was kinda hard to find. But after w...      0\n",
       "2      5.0   734716  For the summer, my brother is lucky enough to ...      0\n",
       "3      5.0   733451  Randomly stopped in here last night (Thursday)...      0\n",
       "4      4.0   733696  YUM this place is so goooood. I only recently ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/raw_data_back_model.csv\",sep=\"\\t\")\n",
    "data = data[['overall', 'ordinal' , 'reviewText','label']]\n",
    "data.head()\n",
    "y=data.iloc[:,3]\n",
    "\n",
    "df_frequency_map={1:1,-1:0}\n",
    "data.label = data.label.map(df_frequency_map)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQRlYTBR8H-3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(0, len(data)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['reviewText'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNmddebaqjYX"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10000,ngram_range=(1, 3))\n",
    "x = cv.fit_transform(corpus)\n",
    "df1 = pd.DataFrame(x.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfT5wRmhmhiX"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(max_features=10000,ngram_range=(1,1))\n",
    "x = cv.fit_transform(corpus)\n",
    "df1 = pd.DataFrame(x.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yTLZEnXIoogC"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df1)\n",
    "scaled_data = scaler.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqctALF450LE"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=25)\n",
    "pca.fit(df1)\n",
    "x_pca = pca.transform(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJ4IxYbS50j_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pca, y, test_size = 0.20, random_state = 25)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gaqvtQQSGf0C",
    "outputId": "774e6d24-1dbe-489d-f797-91e71c499cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(775, 1) (9611, 1)\n"
     ]
    }
   ],
   "source": [
    "auth_series = pd.DataFrame({'label':y_test}) \n",
    "Spam = auth_series[auth_series['label']==1]\n",
    "\n",
    "NotSpam = auth_series[auth_series['label']==0]\n",
    "print(Spam.shape,NotSpam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qTZOy9O8mwZ"
   },
   "outputs": [],
   "source": [
    "# Training model using Naive bayes classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(df1, y, test_size = 0.20, random_state = 25)\n",
    "#X_train_, X_val_, y_train_, y_val_ = train_test_split(X_train_, y_train_, test_size = 0.25, random_state = 25)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier_naive = MultinomialNB().fit(X_train_, y_train_)\n",
    "\n",
    "y_pred_naive=classifier_naive.predict(X_test_)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,brier_score_loss,classification_report\n",
    "\n",
    "cm_naive = confusion_matrix(y_test, y_pred_naive)\n",
    "bl_naive=brier_score_loss(y_test, y_pred_naive)\n",
    "ro_naive=roc_auc_score(y_test, y_pred_naive)\n",
    "acc_naive=accuracy_score(y_test,y_pred_naive)\n",
    "print(cm_naive)\n",
    "print(round(bl_naive,2))\n",
    "print(round(ro_naive,2))\n",
    "print(round(acc_naive,2))\n",
    "print(classification_report(y_test,y_pred_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tx5AONcNytNG"
   },
   "outputs": [],
   "source": [
    "params={\n",
    " \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "    \n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import xgboost\n",
    "classifier=xgboost.XGBClassifier()\n",
    "random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,n_jobs=-1,cv=5,verbose=3)\n",
    "random_search.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lk2O1HldzuZ0"
   },
   "outputs": [],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXYm79Fl8sO3"
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "classifier_xg = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.3,\n",
    "              learning_rate=0.3, max_delta_step=0, max_depth=15,\n",
    "              min_child_weight=3, missing=None, n_estimators=100, n_jobs=1,\n",
    "              nthread=None, objective='binary:logistic', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "              silent=None, subsample=1, verbosity=1)\n",
    "classifier_xg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_xg = classifier_xg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,brier_score_loss,classification_report\n",
    "\n",
    "cm_xg = confusion_matrix(y_test, y_pred_xg)\n",
    "bl_xg=brier_score_loss(y_test, y_pred_xg)\n",
    "ro_xg=roc_auc_score(y_test, y_pred_xg)\n",
    "acc_xg=accuracy_score(y_test,y_pred_xg)\n",
    "print(cm_xg)\n",
    "print(round(bl_xg,2))\n",
    "print(round(ro_xg,2))\n",
    "print(round(acc_xg,2))\n",
    "print(classification_report(y_test,y_pred_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19UIESNU8uQU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "est = RandomForestClassifier(n_jobs=-1)\n",
    "rf_p_dist={'max_depth':[3,5,10,None],\n",
    "              'n_estimators':[10,100,200,300,400,500],\n",
    "              'max_features':randint(1,21),\n",
    "               'criterion':['gini','entropy'],\n",
    "               'bootstrap':[True,False],\n",
    "               'min_samples_leaf':randint(1,4),\n",
    "              }\n",
    "def hypertuning_rscv(est, p_distr, nbr_iter,X,y):\n",
    "    rdmsearch = RandomizedSearchCV(est, param_distributions=p_distr,\n",
    "                                  n_jobs=-1, n_iter=nbr_iter, cv=9)\n",
    "    #CV = Cross-Validation ( here using Stratified KFold CV)\n",
    "    rdmsearch.fit(X,y)\n",
    "    ht_params = rdmsearch.best_params_\n",
    "    ht_score = rdmsearch.best_score_\n",
    "    return ht_params, ht_score\n",
    "\n",
    "rf_parameters, rf_ht_score = hypertuning_rscv(est, rf_p_dist, 40, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWnTj3epWlUu"
   },
   "outputs": [],
   "source": [
    "rf_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "wFz1XZ1KW4rm",
    "outputId": "6b2e6586-3091-40e8-bb70-aff05b1d5153"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_pred_rf = claasifier_rf.predict(X_test)\\n\\nfrom sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,brier_score_loss,classification_report\\n\\ncm_rf = confusion_matrix(y_test, y_pred_rf)\\nbl_rf=brier_score_loss(y_test, y_pred_rf)\\nro_rf=roc_auc_score(y_test, y_pred_rf)\\nacc_rf=accuracy_score(y_test,y_pred_rf)\\nprint(cm_rf)\\nprint(round(bl_rf,2))\\nprint(round(ro_rf,2))\\nprint(round(acc_rf,2))\\nprint(classification_report(y_test,y_pred_rf))'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "claasifier_rf=RandomForestClassifier(n_jobs=-1, n_estimators=400,bootstrap= True,criterion='entropy',max_depth=None,max_features=12,min_samples_leaf= 1)\n",
    "claasifier_rf.fit(df1, y)\n",
    "'''y_pred_rf = claasifier_rf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,brier_score_loss,classification_report\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "bl_rf=brier_score_loss(y_test, y_pred_rf)\n",
    "ro_rf=roc_auc_score(y_test, y_pred_rf)\n",
    "acc_rf=accuracy_score(y_test,y_pred_rf)\n",
    "print(cm_rf)\n",
    "print(round(bl_rf,2))\n",
    "print(round(ro_rf,2))\n",
    "print(round(acc_rf,2))\n",
    "print(classification_report(y_test,y_pred_rf))'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "avo78_ucgZQ-",
    "outputId": "5f20cf40-cb8e-42f9-dc3d-718ff7eddbb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Please enter The text: good pizza.\n",
      "Feature Vector before encoding:\n",
      "  comment_text\n",
      "0  good pizza.\n",
      "   aaa  aback  abandoned  abbaye  ...  zoo  zucchini  zuppa  zutto\n",
      "0  0.0    0.0        0.0     0.0  ...  0.0       0.0    0.0    0.0\n",
      "\n",
      "[1 rows x 10000 columns]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "Text = input(\"Please enter The text: \")\n",
    "data = {'comment_text':[Text]}\n",
    "feature_vector = pd.DataFrame(data)\n",
    "print(\"Feature Vector before encoding:\" )\n",
    "print(feature_vector)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "review = re.sub('[^a-zA-Z]', ' ', feature_vector['comment_text'][0])\n",
    "review = review.lower()\n",
    "review = review.split()\n",
    "    \n",
    "review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "review = ' '.join(review)\n",
    "corpus.append(review)\n",
    "feature_vectors_enc = cv.transform(corpus)\n",
    "df2=pd.DataFrame(feature_vectors_enc.toarray(),columns=cv.get_feature_names())\n",
    "print(df2)\n",
    "result=claasifier_rf.predict(df2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Wl8mhSypoIp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('random_forest.pkl','wb') as f:\n",
    "    pickle.dump(claasifier_rf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjeGBnQT97Vz"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, LeakyReLU, BatchNormalization, Dropout\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.optimizers import adagrad\n",
    "\n",
    "\n",
    "\n",
    "def create_model(layers, activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes,input_dim=X_train.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(0.3))\n",
    "        else:\n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(0.3))\n",
    "            \n",
    "    model.add(Dense(units = 1, kernel_initializer= 'glorot_uniform', activation = 'sigmoid')) # Note: no activation beyond this point\n",
    "    \n",
    "    optimizer = adagrad(lr=0.01)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "\n",
    "layers = [[20], [40, 20], [45, 30, 15]]\n",
    "activations = ['sigmoid', 'relu']\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[30])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=5)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "[grid_result.best_score_,grid_result.best_params_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PgTt0TcDBjM"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_model():\n",
    "  # Initialising the ANN\n",
    "  classifier_ann = Sequential()\n",
    "\n",
    "  # Adding the input layer and the first hidden layer\n",
    "  classifier_ann.add(Dense(output_dim = 40,activation='sigmoid',input_dim = 20))\n",
    "  classifier_ann.add(Dropout(0.3))\n",
    "\n",
    "  # Adding the second hidden layer\n",
    "  classifier_ann.add(Dense(output_dim = 20,activation='sigmoid'))\n",
    "  classifier_ann.add(Dropout(0.3))\n",
    "  # Adding the output layer\n",
    "  classifier_ann.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "  # Compiling the ANN\n",
    "  optimizer = RMSprop(lr=0.01)\n",
    "  classifier_ann.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "  return classifier_ann\n",
    "\n",
    "classifier_ann=KerasClassifier(build_fn=create_model,validation_split=0.33, epochs=2, batch_size=128)\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier_ann.fit(X_train, y_train)\n",
    "\n",
    "# list all data in history\n",
    "print(model_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['acc'])\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_ann = classifier_ann.predict(X_test)\n",
    "y_pred_ann = (y_pred_ann > 0.5)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,brier_score_loss,classification_report\n",
    "\n",
    "cm_ann = confusion_matrix(y_test, y_pred_ann)\n",
    "bl_ann=brier_score_loss(y_test, y_pred_ann)\n",
    "ro_ann=roc_auc_score(y_test, y_pred_ann)\n",
    "acc_ann=accuracy_score(y_test,y_pred_ann)\n",
    "print(cm_ann)\n",
    "print(round(bl_ann,2))\n",
    "print(round(ro_ann,2))\n",
    "print(round(acc_ann,2))\n",
    "print(classification_report(y_test,y_pred_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRAU9ffYKT5k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(X_train)\n",
    "testing_set_scaled=sc.transform(X_test)\n",
    "\n",
    "x_train = np.reshape(training_set_scaled, (training_set_scaled.shape[0], training_set_scaled.shape[1], 1))\n",
    "\n",
    "x_test = np.reshape(testing_set_scaled, (testing_set_scaled.shape[0], testing_set_scaled.shape[1], 1))\n",
    "\n",
    "#validation_set_scaled=sc.transform(X_val)\n",
    "\n",
    "#x_val = np.reshape(validation_set_scaled, (validation_set_scaled.shape[0], validation_set_scaled.shape[1], 1))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialising the RNN\n",
    "def create_model():\n",
    "\n",
    "  classifier_lstm = Sequential()\n",
    "\n",
    "  # Adding the first LSTM layer and some Dropout regularisation\n",
    "  classifier_lstm.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
    "  classifier_lstm.add(Dropout(0.2))\n",
    "\n",
    "  # Adding a second LSTM layer and some Dropout regularisation\n",
    "  classifier_lstm.add(LSTM(units = 50, return_sequences = True))\n",
    "  classifier_lstm.add(Dropout(0.2))\n",
    "\n",
    "  # Adding a third LSTM layer and some Dropout regularisation\n",
    "  classifier_lstm.add(LSTM(units = 50, return_sequences = True))\n",
    "  classifier_lstm.add(Dropout(0.2))\n",
    "\n",
    "  # Adding a fourth LSTM layer and some Dropout regularisation\n",
    "  classifier_lstm.add(LSTM(units = 50))\n",
    "  classifier_lstm.add(Dropout(0.2))\n",
    "\n",
    "  # Adding the output layer\n",
    "  classifier_lstm.add(Dense(units = 1))\n",
    "\n",
    "  # Compiling the RNN\n",
    "  classifier_lstm.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "  return classifier_lstm\n",
    "\n",
    "classifier_lstm=KerasClassifier(build_fn=create_model,validation_split=0.33,epochs=1, batch_size=128)\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model_history=classifier_lstm.fit(x_train, y_train)\n",
    "\n",
    "# list all data in history\n",
    "print(model_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['acc'])\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_lstm = classifier_lstm.predict(x_test)\n",
    "y_pred_lstm = (y_pred_lstm > 0.5)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,brier_score_loss,classification_report\n",
    "\n",
    "cm_lstm = confusion_matrix(y_test, y_pred_lstm)\n",
    "bl_lstm=brier_score_loss(y_test, y_pred_lstm)\n",
    "ro_lstm=roc_auc_score(y_test, y_pred_lstm)\n",
    "acc_lstm=accuracy_score(y_test,y_pred_lstm)\n",
    "print(cm_lstm)\n",
    "print(round(bl_lstm,2))\n",
    "print(round(ro_lstm,2))\n",
    "print(round(acc_lstm,2))\n",
    "print(classification_report(y_test,y_pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7X10go9KPy6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC()\n",
    "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mE8EItwZAP7"
   },
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = 'rbf', gamma=0.7)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val=cross_val_score(classifier,x_pca,y,cv=10,scoring='accuracy').mean()\n",
    "\n",
    "print(cross_val)\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhaAnkgfxSof"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline\n",
    "\n",
    "error_rate = []\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3KVzDBXxUz1"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# NOW WITH K=2\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "classifier_knn.fit(X_train,y_train)\n",
    "y_pred_knn = classifier_knn.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,accuracy_score,brier_score_loss,classification_report\n",
    "\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "bl_knn=brier_score_loss(y_test, y_pred_knn)\n",
    "ro_knn=roc_auc_score(y_test, y_pred_knn)\n",
    "acc_knn=accuracy_score(y_test,y_pred_knn)\n",
    "print(cm_knn)\n",
    "print(round(bl_knn,2))\n",
    "print(round(ro_knn,2))\n",
    "print(round(acc_knn,2))\n",
    "print(classification_report(y_test,y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "4hY35_uB57K4",
    "outputId": "2eb1000e-dad4-481a-dd18-1561f310162a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZTcWEwN69Ct"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from google.colab import files\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrixes with ngram range(1,3) using k=7,a=10\",fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Xtreme Gradient Boosting\")\n",
    "sns.heatmap(cm_xg,annot=True,cmap=\"Greys\",fmt=\"d\",cbar=False)\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"K Nearest Neighbors\")\n",
    "sns.heatmap(cm_knn,annot=True,cmap=\"Purples\",fmt=\"d\",cbar=False)\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.title(\"Artificial Neural Network\")\n",
    "sns.heatmap(cm_ann,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Naive Bayes   \")\n",
    "sns.heatmap(cm_naive,annot=True,cmap=\"Greens\",fmt=\"d\",cbar=False)\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Long Short Term Memory\")\n",
    "sns.heatmap(cm_lstm,annot=True,cmap=\"Oranges\",fmt=\"d\",cbar=False)\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"Random Forest\")\n",
    "sns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n",
    "fig.savefig('Confusion Matrixes with ngram range(1,1) Using Count Vectroizer.png', dpi=fig.dpi)\n",
    "files.download('Confusion Matrixes with ngram range(1,1) Using Count Vectroizer.png') \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tx1YzxaqZt1R"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from google.colab import files\n",
    "\n",
    "y_pred_ann_ = classifier_ann.predict_proba(X_test)[:,1]\n",
    "fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred_ann_)\n",
    "auc_ann = auc(fpr_ann, tpr_ann)\n",
    "\n",
    "y_pred_lstm_ = classifier_lstm.predict_proba(x_test)[:,1]\n",
    "fpr_lstm, tpr_lstm, thresholds_lstm = roc_curve(y_test, y_pred_lstm_)\n",
    "auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
    "\n",
    "'''y_pred_ann = classifier_ann.predict(X_test).ravel()\n",
    "fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred_ann_)\n",
    "auc_ann = auc(fpr_ann, tpr_ann)\n",
    "\n",
    "y_pred_lstm = classifier_lstm.predict(x_test).ravel()\n",
    "fpr_lstm, tpr_lstm, thresholds_lstm = roc_curve(y_test, y_pred_lstm)\n",
    "auc_lstm = auc(fpr_lstm, tpr_lstm)'''\n",
    "\n",
    "y_pred_rf_ = claasifier_rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf_)\n",
    "auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "y_pred_xg_ = classifier_xg.predict_proba(X_test)[:, 1]\n",
    "fpr_xg, tpr_xg, thresholds_xg = roc_curve(y_test, y_pred_xg_)\n",
    "auc_xg = auc(fpr_xg, tpr_xg)\n",
    "\n",
    "y_pred_naive_ = classifier_naive.predict_proba(X_test_)[:, 1]\n",
    "fpr_naive, tpr_naive, thresholds_naive = roc_curve(y_test_, y_pred_naive_)\n",
    "auc_naive = auc(fpr_naive, tpr_naive)\n",
    "\n",
    "y_pred_knn_ = classifier_knn.predict_proba(X_test)[:, 1]\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_pred_knn_)\n",
    "auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "fig=plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_ann, tpr_ann, label='ANN (area = {:.3f})'.format(auc_ann))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.plot(fpr_lstm, tpr_lstm, label='LSTM (area = {:.3f})'.format(auc_lstm))\n",
    "plt.plot(fpr_naive, tpr_naive, label='NB (area = {:.3f})'.format(auc_naive))\n",
    "plt.plot(fpr_xg, tpr_xg, label='XGB (area = {:.3f})'.format(auc_xg))\n",
    "plt.plot(fpr_knn, tpr_knn, label='KNN (area = {:.3f})'.format(auc_knn))\n",
    "plt.xlabel('False positive rate (Specificity)')\n",
    "plt.ylabel('True positive rate (Sensitivity)')\n",
    "plt.title('ROC curve with ngram range(1,3) using k=7,a=10')\n",
    "plt.legend(loc='best')\n",
    "fig.savefig('ROC curve with ngram range(1,2).png', dpi=fig.dpi)\n",
    "files.download('ROC curve with ngram range(1,2).png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHPxWdizuuAF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import calibration_curve\n",
    "from google.colab import files\n",
    "\n",
    "# #############################################################################\n",
    "# Plot calibration plots\n",
    "\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "for clf, name in [(classifier_xg, 'Xg Boost'),\n",
    "                  (classifier_naive, 'Naive Bayes'),\n",
    "                  (classifier_ann, 'K Nearest Negihbors'),\n",
    "                  (claasifier_rf, 'Random Forest'),\n",
    "                  (classifier_ann,'Artifical Neural Network'),\n",
    "                  (classifier_lstm,'LSTM')\n",
    "                  ]:\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        if name=='Naive Bayes':\n",
    "          prob_pos = clf.predict_proba(X_test_)[:, 1]\n",
    "        elif name=='LSTM':\n",
    "          prob_pos = clf.predict_proba(x_test)[:, 1]\n",
    "          prob_pos=np.interp(a, (a.min(), a.max()), (0, +1))\n",
    "        else:\n",
    "          prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "    else:  # use decision function\n",
    "        prob_pos = clf.decision_function(X_test)\n",
    "        prob_pos = \\\n",
    "            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    fraction_of_positives, mean_predicted_value = \\\n",
    "        calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "             label=\"%s\" % (name, ))\n",
    "\n",
    "    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "             histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots (reliability curve) with ngram range(1,1) using k=7,a=10')\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('Calibration curve with ngram range(1,1).png', dpi=fig.dpi)\n",
    "files.download('Calibration curve with ngram range(1,1).png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKSyE1_FnIoy"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.calibration import (CalibratedClassifierCV,\n",
    "                                 _CalibratedClassifier,\n",
    "                                 _SigmoidCalibration)\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, label_binarize\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from google.colab import files\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def compute_score(model, X, y, verbose=True, round_digits=3):\n",
    "    import sklearn.metrics as metrics\n",
    "    y_score = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    auc = round(metrics.roc_auc_score(y, y_score), round_digits)\n",
    "    log_loss = round(metrics.log_loss(y, y_score), round_digits)\n",
    "    brier = round(metrics.brier_score_loss(y, y_score), round_digits)\n",
    "\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(y, y_score)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    mask = ~np.isnan(f1)\n",
    "    f1 = f1[mask]\n",
    "    precision = precision[mask]\n",
    "    recall = recall[mask]\n",
    "\n",
    "    best_index = np.argmax(f1)\n",
    "    precision = round(precision[best_index], round_digits)\n",
    "    recall = round(recall[best_index], round_digits)\n",
    "    f1 = round(f1[best_index], round_digits)\n",
    "\n",
    "    if verbose:\n",
    "        print('auc: ', auc)\n",
    "        print('precision: ', precision)\n",
    "        print('recall: ', recall)\n",
    "        print('f1: ', f1)\n",
    "        print('brier: ', brier)\n",
    "        print('log_loss: ', log_loss)\n",
    "\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'log_loss': log_loss,\n",
    "        'brier': brier\n",
    "    }\n",
    "\n",
    "class CalibratedPrefitClassifier(CalibratedClassifierCV):\n",
    "\n",
    "    def __init__(self, base_estimator, method):\n",
    "        super().__init__(base_estimator, method, cv='prefit')\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Fit the calibrated model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "        # scikit-learn has a check_X_y here that will convert our dataframe\n",
    "        # to a numpy array, remove that part to prevent the conversion\n",
    "\n",
    "        le = LabelBinarizer().fit(y)\n",
    "        self.classes_ = le.classes_\n",
    "\n",
    "        # the main difference from scikit-learn is that our cv parameter will\n",
    "        # always be 'prefit', hence we don't need to handle the cross validation case,\n",
    "        # we still honor that original attribute list type, even though it isn't necessary\n",
    "        self.calibrated_classifiers_ = []\n",
    "        calibrated_classifier = CalibratedClassifier(self.base_estimator, method=self.method)\n",
    "        if sample_weight is not None:\n",
    "            calibrated_classifier.fit(X, y, sample_weight)\n",
    "        else:\n",
    "            calibrated_classifier.fit(X, y)\n",
    "\n",
    "        self.calibrated_classifiers_.append(calibrated_classifier)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Posterior probabilities of classification\n",
    "\n",
    "        This function returns posterior probabilities of classification\n",
    "        according to each class on an array of test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba : array, shape (n_samples, n_classes)\n",
    "            The predicted probas.\n",
    "        \"\"\"\n",
    "        # the check_is_fitted method might change to not needing to pass\n",
    "        # the attribute that we're checking for in the future\n",
    "        check_is_fitted(self, ['classes_', 'calibrated_classifiers_'])\n",
    "\n",
    "        # Compute the arithmetic mean of the predictions of the calibrated\n",
    "        # classifiers\n",
    "        mean_proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        for calibrated_classifier in self.calibrated_classifiers_:\n",
    "            proba = calibrated_classifier.predict_proba(X)\n",
    "            mean_proba += proba\n",
    "\n",
    "        mean_proba /= len(self.calibrated_classifiers_)\n",
    "        return mean_proba\n",
    "\n",
    "\n",
    "class CalibratedClassifier(_CalibratedClassifier):\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Calibrate the fitted model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "        self.label_encoder_ = LabelEncoder()\n",
    "        if self.classes is None:\n",
    "            self.label_encoder_.fit(y)\n",
    "        else:\n",
    "            self.label_encoder_.fit(self.classes)\n",
    "\n",
    "        self.classes_ = self.label_encoder_.classes_\n",
    "        Y = label_binarize(y, self.classes_)\n",
    "\n",
    "        # df is the predicted score/probability of the base_estimator\n",
    "        df, idx_pos_class = self._preproc(X)\n",
    "\n",
    "        self.calibrators_ = []\n",
    "        for k, this_df in zip(idx_pos_class, df.T):\n",
    "            if self.method == 'isotonic':\n",
    "                # coerce the predicted score to a double to prevent type error\n",
    "                this_df = this_df.astype(np.float64)\n",
    "                calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "            elif self.method == 'sigmoid':\n",
    "                calibrator = _SigmoidCalibration()\n",
    "            else:\n",
    "                raise ValueError('method should be \"sigmoid\" or '\n",
    "                                 '\"isotonic\". Got %s.' % self.method)\n",
    "            calibrator.fit(this_df, Y[:, k], sample_weight)\n",
    "            self.calibrators_.append(calibrator)\n",
    "\n",
    "        return self\n",
    "\n",
    "def plot_calibration_curve(estimators_names, X, y):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "    estimator_metrics = []\n",
    "    for estimator, name in estimators_names:\n",
    "        proba = estimator.predict_proba(X)[:, 1]\n",
    "        #proba=np.interp(proba, (proba.min(), proba.max()), (0, +1))\n",
    "        prob_true, prob_pred = calibration_curve(y, proba, n_bins=10)\n",
    "\n",
    "\n",
    "        #metric_dict = compute_score(estimator, X, y, verbose=False)\n",
    "        #metric_dict['name'] = name\n",
    "        #estimator_metrics.append(metric_dict)\n",
    "\n",
    "        ax1.plot(prob_pred, prob_true, 's-',label=name)\n",
    "        ax2.hist(proba, range=(0, 1), bins=10, label=name, histtype='step', lw=2)\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], 'k:')\n",
    "\n",
    "    ax1.set_xlabel('Predicted Probability')\n",
    "    ax1.set_ylabel('True Probability')\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.set_title('Calibration Plots (Reliability Curve)')\n",
    "\n",
    "    #Naive Bayes\n",
    "    #Random Forest\n",
    "    #Xtreme Gradient Boosting\n",
    "    #Artifical Neural Network\n",
    "    #Long Short Term Memory\n",
    "    #K Nearest Neighbors\n",
    "    ax2.set_xlabel('Predicted scores')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax1.set_title('Calibration plot (reliability curve) of K Nearest Neighbors with ngram range(1,3) using k=7,a=10')\n",
    "    ax2.legend(loc='upper center', ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #df_metrics = pd.DataFrame(estimator_metrics)\n",
    "    return fig\n",
    "\n",
    "xgb_sigmoid = CalibratedPrefitClassifier(classifier_knn, method='sigmoid')\n",
    "xgb_sigmoid.fit(X_val, y_val)\n",
    "\n",
    "xgb_isotonic = CalibratedPrefitClassifier(classifier_knn, method='isotonic')\n",
    "xgb_isotonic.fit(X_val, y_val)\n",
    "estimators_names = [\n",
    "    (classifier_knn, 'Uncalibrated'),\n",
    "    (xgb_sigmoid, 'Calibrated(Platt)'),\n",
    "    (xgb_isotonic, 'Calibrated(Isotonic)')\n",
    "]\n",
    "fig = plot_calibration_curve(estimators_names, X_test, y_test)\n",
    "fig.savefig('Calibration.png', dpi=fig.dpi)\n",
    "files.download('Calibration.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcwdoGMfCp7f"
   },
   "outputs": [],
   "source": [
    "3# precision-recall curve and f1\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "\n",
    "ann_precision, ann_recall, _ = precision_recall_curve(y_test, y_pred_ann_)\n",
    "lstm_precision, lstm_recall, _ = precision_recall_curve(y_test, y_pred_lstm_)\n",
    "knn_precision, knn_recall, _ = precision_recall_curve(y_test, y_pred_knn_)\n",
    "nb_precision, nb_recall, _ = precision_recall_curve(y_test_, y_pred_naive_)\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, y_pred_rf_)\n",
    "xg_precision, xg_recall, _ = precision_recall_curve(y_test, y_pred_xg)\n",
    "# plot the precision-recall curves\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(ann_recall, ann_precision, marker='.', label='ANN')\n",
    "plt.plot(lstm_recall, lstm_precision, marker='.', label='LSTM')\n",
    "plt.plot(knn_recall, knn_precision, marker='.', label='KNN')\n",
    "plt.plot(nb_recall, nb_precision, marker='.', label='Naive Bayes')\n",
    "plt.plot(xg_recall, xg_precision, marker='.', label='Xg Boost')\n",
    "plt.plot(rf_recall, rf_precision, marker='.', label='Random Forest')\n",
    "\n",
    "# axis labels\n",
    "fig.suptitle('Precision Recall Curve with ngram range(1,3) using k=7,a=10',fontsize=20)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "fig.savefig('PR.png', dpi=fig.dpi)\n",
    "files.download('PR.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSbjcqPqx5XU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "data = pd.read_excel('/content/drive/My Drive/Colab Notebooks/Daraz_Distributed_Soft_Label_Final.xlsx',usecols=['Review', 'Label'])\n",
    "\n",
    "y=data.iloc[:,1]\n",
    "df_frequency_map={1:1,-1:0}\n",
    "data.Label = data.Label.map(df_frequency_map)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZZWhQlOC4hB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(0, len(data)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_GE4eyn6k8x"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(max_features=10000)\n",
    "x = cv.fit_transform(corpus)\n",
    "df1 = pd.DataFrame(x.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmzmOun56uRI"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df1)\n",
    "scaled_data = scaler.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-c_EKlgv6zPt"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orZryQCE62QZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pca, y, test_size = 0.10, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXQFcOCt71s9"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 40,activation='sigmoid',input_dim = 20))\n",
    "classifier.add(Dropout(0.3))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 20,activation='sigmoid'))\n",
    "classifier.add(Dropout(0.3))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "'''model_history=classifier.fit(X_train, y_train,validation_split=0.1,batch_size = 128, nb_epoch = 30)\n",
    "\n",
    "# list all data in history\n",
    "print(model_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['acc'])\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_pred,y_test)\n",
    "\n",
    "print(cm)\n",
    "print(score)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ST8peRPR8NWW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(X_train)\n",
    "testing_set_scaled=sc.transform(X_test)\n",
    "\n",
    "x_train = np.reshape(training_set_scaled, (training_set_scaled.shape[0], training_set_scaled.shape[1], 1))\n",
    "\n",
    "x_test = np.reshape(testing_set_scaled, (testing_set_scaled.shape[0], testing_set_scaled.shape[1], 1))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialising the RNN\n",
    "classifier1 = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "classifier1.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
    "classifier1.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "classifier1.add(LSTM(units = 50, return_sequences = True))\n",
    "classifier1.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "classifier1.add(LSTM(units = 50, return_sequences = True))\n",
    "classifier1.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "classifier1.add(LSTM(units = 50))\n",
    "classifier1.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier1.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "classifier1.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model_history=classifier1.fit(x_train, y_train,validation_split=0.1,epochs = 10, batch_size = 128)\n",
    "\n",
    "# list all data in history\n",
    "print(model_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['acc'])\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier1.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_pred,y_test)\n",
    "\n",
    "print(score)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yk995FaEmsif"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score,accuracy_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_calibration_curve(est, name, fig_index):\n",
    "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "    # Calibrated with isotonic calibration\n",
    "    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n",
    "\n",
    "    # Calibrated with sigmoid calibration\n",
    "    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n",
    "\n",
    "    # Logistic regression with no calibration as baseline\n",
    "    lr = LogisticRegression(C=1.)\n",
    "\n",
    "    fig = plt.figure(fig_index, figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    for clf, name in [(lr, 'Logistic'),\n",
    "                      (est, name),\n",
    "                      (isotonic, name + ' + Isotonic'),\n",
    "                      (sigmoid, name + ' + Sigmoid')]:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        else:  # use decision function\n",
    "            prob_pos = clf.decision_function(X_test)\n",
    "            prob_pos = \\\n",
    "                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\acc: %1.3f\" % accuracy_score(y_test,y_pred))\n",
    "        print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "        print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n",
    "        print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n",
    "        print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = \\\n",
    "            calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "                 label=\"%s (%1.3f)\" % (name, clf_score))\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "                 histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "    fig.savefig('temp.png', dpi=fig.dpi)\n",
    "    files.download(\"temp.png\") \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Plot calibration curve for Linear SVC\n",
    "plot_calibration_curve(LinearSVC(), \"ANN\", 2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1IThxOBpnhf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
